{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandits in probability theory are a set of problems that essentially aim to optimize long term returns when given K potential options. Specifically the name is a good adage to summarize this problem, given a gambler facing a row of multiple single arm bandits (slot machines, pokies) what is the best playing strategy to optimize returns? In other words, given a finite set of resources (the gamblers money) that will be spent among competing options, which at the time of playing may only be partially known and may be better determined as further rounds are played what is the best way to maximize his returns. This leads into the idea of exploration vs exploitation, should the gambler keep playing one machine until it pays, should he distribute his money evenly among the bandits and hope one pays off, or is there a better strategy that will help maximize his returns? We can assume that if one machine is consistently paying the gambler should be allocating his money to it, but what if there are other machines that could be potentially paying off the same or if not more that he is not focused on? The take away here is how do we determine the optimal way to explore and exploit to maximize returns.\n",
    "\n",
    "Let's take this example, the gambler starts by even distributing his money among K machines. When enough rounds are played and there is a machine that performs better than the rest the gambler takes portions from other machines and dynamically allocates it to this one, whilst periodically checking to see if the other machines are becoming more promising. Here the gambler is continuing to exploit the paying machine whilst continuing to explore for other bandits that will potentially offer returns as well.\n",
    "\n",
    "We can implement some code to demonstrate this exact scenario, but first let's dig a little bit into the theory of MAB and Upper Confidence Bound solution to the multi-armed bandit problem and how it solves the exloration vs exploitation dilemma.\n",
    "\n",
    "A multi-armed bandit can be thought of as a tuple $\\langle\\mathcal{A}, \\mathcal{R}\\rangle$. \n",
    "\n",
    "Where by $\\mathcal{A}$ in our example can be seen as a set of actions or interactions ie. with our bandits. At each step t the agent selects an action $a_{t} \\in \\mathcal{A}$ . \n",
    "\n",
    "$\\mathcal{R}^{a}(r)=\\mathbb{P}[r | a]$ is an unknown probability distribution of potential rewards, which we can think of as our expected payoffs. In our interactions, the bandit that gets played will generate some reward $r_{t} \\sim \\mathcal{R}^{a_{t}}$ our aim in this process is to maximise cumulative reward $\\sum_{\\tau=1}^{t} r_{\\tau}$ - pick as many winners as possible.\n",
    "\n",
    "The action-value is the mean reward for action a, $Q(a)=\\mathbb{E}[r | a]$.\n",
    "\n",
    "This process can be viewed a simplified version of the Markov decision process, given the absence of state.\n",
    "\n",
    "Now how do we determine the best course of action? We know that we want to pick the most profitable machine, but if we were to focus on one, how would we account for the potential losses from ignoring equally or more profitable machines? This is where a concept called regret comes into the picture. Our aim in this exercise is not only to maximize reward (profits) but also minimize regret (lost optimal opportunities).\n",
    "\n",
    "The optimal value is described as $V^{*}$ is $V^{*}=Q\\left(a^{*}\\right)=\\max _{a \\in \\mathcal{A}} Q(a)$\n",
    "\n",
    "The incured regret for one time step is $I_{t}=\\mathbb{E}\\left[V^{*}-Q\\left(a_{t}\\right)\\right]$\n",
    "\n",
    "The total regret for the total opportunity loss $L_{t}=\\mathbb{E}\\left[\\sum_{\\tau=1}^{t} V^{*}-Q\\left(a_{\\tau}\\right)\\right]$\n",
    "\n",
    "Therefore our optimal way to get the best return is to maximise cumulative reward and to minimise total regret.\n",
    "\n",
    "### Optimism in the face of uncertainty\n",
    "\n",
    "In UCB the more uncertain we are about a given action the more important it is to explore it as it could turn out to be the best action.\n",
    "\n",
    "UCB measures the potenital for a given action by estimating the upper confidence $\\hat{U}_t(a)$ for each action value, such that $Q(a) \\leq \\hat{Q}_t(a) + \\hat{U}_t(a)$.\n",
    "\n",
    "$\\hat{U}_t(a)$ is a function of ${N_{t}(a)}$, and with a small ${N_{t}(a)}$ we have a large $\\hat{U}_t(a)$ and thus we have a high uncertainty of our estimation. Conversely if we have a large ${N_{t}(a)}$ we have a small $\\hat{U}_t(a)$ and our estimate is more accurate.\n",
    "\n",
    "UCB takes a gready approach and will always select the action that maximizes UCB: $a_t = argmax_{a \\in \\mathcal{A}} \\hat{Q}_t(a) + \\hat{U}_t(a)$\n",
    "\n",
    "So now we must estimate UCB and to this we need to introduce Hoeffdingâ€™s Inequality.\n",
    "\n",
    "$a_{t}=\\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} Q(a)+\\sqrt{\\frac{2 \\log t}{N_{t}(a)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"Class for running a toy example of MAB UCB1 problem.\n",
    "    \n",
    "    :param money: How much money the gambler has to spend.\n",
    "    :type money: int\n",
    "    :param n_bandits: How many bandits there are to play.\n",
    "    :type n_bandits: int\n",
    "    \"\"\"\n",
    "    def __init__(self, money, n_bandits):\n",
    "        self.money = money\n",
    "        self.n_bandits = n_bandits\n",
    "        self.bandits_played = []\n",
    "        self.bandits_returns = {x:0 for x in range(self.n_bandits)}\n",
    "        self.n_selections = {x:0 for x in range(self.n_bandits)}\n",
    "        self.total_return = 0\n",
    "        self.bounds = {x:[] for x in range(self.n_bandits)}\n",
    "        \n",
    "    def calc_ucb(self, i, t):\n",
    "        if self.n_selections[i] > 0:\n",
    "            Qa = self.bandits_returns[i] / self.n_selections[i]\n",
    "            Ut = math.sqrt(2 * math.log(t + 1) / self.n_selections[i])\n",
    "            ub = Qa + Ut\n",
    "            print('Upper bound: ', ub)\n",
    "            self.bounds[i].append(ub)\n",
    "        else:\n",
    "            ub = 10\n",
    "            self.bounds[i].append(ub)\n",
    "        return {i:ub}\n",
    "        \n",
    "    def play_round(self, t):\n",
    "        res = {k: v for d in [self.calc_ucb(x, t) for x in range(self.n_bandits)] for k, v in d.items()}\n",
    "        bandit = max(res, key=lambda v: res[v])\n",
    "        self.bandits_played.append(bandit)\n",
    "        print('Bandit: ', bandit)\n",
    "        self.n_selections[bandit] += 1\n",
    "        cash_return = int(input())\n",
    "        self.bandits_returns[bandit] += cash_return\n",
    "        self.total_return += cash_return\n",
    "        \n",
    "    def run(self):\n",
    "        [self.play_round(x) for x in range(self.money)]\n",
    "        print(f'Bandit {max(self.bandits_returns, key=lambda v: self.bandits_returns[v])} was the optimal machine.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB1(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.177410022515475\n",
      "Bandit:  1\n",
      "0\n",
      "Upper bound:  2.4823038073675114\n",
      "Upper bound:  1.4823038073675112\n",
      "Bandit:  2\n",
      "0\n",
      "Upper bound:  2.6651092223153956\n",
      "Upper bound:  1.6651092223153954\n",
      "Upper bound:  1.6651092223153954\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.26863624117952\n",
      "Upper bound:  1.7941225779941015\n",
      "Upper bound:  1.7941225779941015\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.0929347248663586\n",
      "Upper bound:  1.8930184728248454\n",
      "Upper bound:  1.8930184728248454\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  1.9863848511243756\n",
      "Upper bound:  1.9727697022487511\n",
      "Upper bound:  1.9727697022487511\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  1.9120178817720266\n",
      "Upper bound:  2.039333980337618\n",
      "Upper bound:  2.039333980337618\n",
      "Bandit:  1\n",
      "0\n",
      "Upper bound:  1.9374912431241627\n",
      "Upper bound:  1.4823038073675112\n",
      "Upper bound:  2.09629414793641\n",
      "Bandit:  2\n",
      "0\n",
      "Upper bound:  1.9597051824376162\n",
      "Upper bound:  1.5174271293851465\n",
      "Upper bound:  1.5174271293851465\n",
      "Bandit:  0\n",
      "1\n",
      "Bandit 0 was the optimal machine.\n"
     ]
    }
   ],
   "source": [
    "ucb.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
