{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandits in probability theory are a set of problems that essentially aim to optimize long term returns when given K potential options. Specifically the name is a good adage to summarize this problem, given a gambler facing a row of multiple single arm bandits (slot machines, pokies) what is the best playing strategy to optimize returns? In other words, given a finite set of resources (the gamblers money) that will be spent among competing options, which at the time of playing may only be partially known and may be better determined as further rounds are played what is the best way to maximize his returns. This leads into the idea of exploration vs exploitation, should the gambler keep playing one machine until it pays, should he distribute his money evenly among the bandits and hope one pays off, or is there a better strategy that will help maximize his returns? We can assume that if one machine is consistently paying the gambler should be allocating his money to it, but what if there are other machines that could be potentially paying off the same or if not more that he is not focused on? The take away here is how do we determine the optimal way to explore and exploit to maximize returns.\n",
    "\n",
    "Let's take this example, the gambler starts by even distributing his money among K machines. When enough rounds are played and there is a machine that performs better than the rest the gambler takes portions from other machines and dynamically allocates it to this one, whilst periodically checking to see if the other machines are becoming more promising. Here the gambler is continuing to exploit the paying machine whilst continuing to explore for other bandits that will potentially offer returns as well.\n",
    "\n",
    "We can implement some code to demonstrate this exact scenario, but first let's dig a little bit into the theory of MAB and Upper Confidence Bound solution to the multi-armed bandit problem and how it solves the exloration vs exploitation dilemma.\n",
    "\n",
    "A multi-armed bandit can be thought of as a tuple $\\langle\\mathcal{A}, \\mathcal{R}\\rangle$. \n",
    "\n",
    "Where by $\\mathcal{A}$ in our example can be seen as a set of actions or interactions ie. with our bandits. At each step t the agent selects an action $a_{t} \\in \\mathcal{A}$ . \n",
    "\n",
    "$\\mathcal{R}^{a}(r)=\\mathbb{P}[r | a]$ is an unknown probability distribution of potential rewards, which we can think of as our expected payoffs. In our interactions, the bandit that gets played will generate some reward $r_{t} \\sim \\mathcal{R}^{a_{t}}$ our aim in this process is to maximise cumulative reward $\\sum_{\\tau=1}^{t} r_{\\tau}$ - pick as many winners as possible.\n",
    "\n",
    "The action-value is the mean reward for action a, $Q(a)=\\mathbb{E}[r | a]$.\n",
    "\n",
    "This process can be viewed a simplified version of the Markov decision process, given the absence of state.\n",
    "\n",
    "Now how do we determine the best course of action? We know that we want to pick the most profitable machine, but if we were to focus on one, how would we account for the potential losses from ignoring equally or more profitable machines? This is where a concept called regret comes into the picture. Our aim in this exercise is not only to maximize reward (profits) but also minimize regret (lost optimal opportunities).\n",
    "\n",
    "The optimal value is described as $V^{*}$ is $V^{*}=Q\\left(a^{*}\\right)=\\max _{a \\in \\mathcal{A}} Q(a)$\n",
    "\n",
    "The incured regret for one time step is $I_{t}=\\mathbb{E}\\left[V^{*}-Q\\left(a_{t}\\right)\\right]$\n",
    "\n",
    "The total regret for the total opportunity loss $L_{t}=\\mathbb{E}\\left[\\sum_{\\tau=1}^{t} V^{*}-Q\\left(a_{\\tau}\\right)\\right]$\n",
    "\n",
    "Therefore our optimal way to get the best return is to maximise cumulative reward and to minimise total regret.\n",
    "\n",
    "### Optimism in the face of uncertainty\n",
    "\n",
    "In UCB the more uncertain we are about a given action the more important it is to explore it as it could turn out to be the best action.\n",
    "\n",
    "UCB measures the potenital for a given action by estimating the upper confidence $\\hat{U}_t(a)$ for each action value, such that $Q(a) \\leq \\hat{Q}_t(a) + \\hat{U}_t(a)$.\n",
    "\n",
    "$\\hat{U}_t(a)$ is a function of ${N_{t}(a)}$, and with a small ${N_{t}(a)}$ we have a large $\\hat{U}_t(a)$ and thus we have a high uncertainty of our estimation. Conversely if we have a large ${N_{t}(a)}$ we have a small $\\hat{U}_t(a)$ and our estimate is more accurate.\n",
    "\n",
    "UCB takes a gready approach and will always select the action that maximizes UCB: $a_t = argmax_{a \\in \\mathcal{A}} \\hat{Q}_t(a) + \\hat{U}_t(a)$\n",
    "\n",
    "So now we must estimate UCB and to this we need to introduce Hoeffding’s Inequality.\n",
    "\n",
    "Hoeffding’s Inequality is an important technique in learning theory. H.I basically provides an upper bound on the probability that the sample mean of some distribution deviates from the true mean by more that a certain amount. \n",
    "\n",
    "In our application, let's assume that $X_1, \\dots, X_t$ are independent and identically distributed random variables bound between 0 and 1.\n",
    "\n",
    "The sample mean is: $\\overline{X}_t = \\frac{1}{t}\\sum_{\\tau=1}^t X_\\tau$ \n",
    "\n",
    "Then we have: $\\mathbb{P} [ \\mathbb{E}[X] > \\overline{X}_t + u] \\leq e^{-2tu^2}$\n",
    "\n",
    "Now, if we apply this to our problem, for a given acion a we will have: $\\mathbb{P} [ Q(a) > \\hat{Q}_t(a) + U_t(a)] \\leq e^{-2t{U_t(a)}^2}$\n",
    "\n",
    "Where:\n",
    "-  $Q(a)$ is our true mean.\n",
    "-  $\\hat{Q}_t(a)$ is our sample mean.\n",
    "-  $U_t(a)$ is our upper confidence bound.\n",
    "\n",
    "So we are basically making the best guess we can, given that each of our $a$'s will have different rewards we can use this inequality to pick a bound so the probability that the true value will exceed UCB.\n",
    "\n",
    "Now we can solve for $U_t(a)$ by setting $e^{-2t U_t(a)^2} = p$ and solving: $U_t(a) = \\sqrt{\\frac{-\\log p}{2 N_t(a)}}$\n",
    "\n",
    "Now as we observe the rewards as we make a number of actions we can begin to reduce $p$, if we set $p = t^{-4}$ we get $U_t(a) = \\sqrt{\\frac{2 \\log t}{N_t(a)}}$ and this ensures we select optimal action as t → ∞.\n",
    "\n",
    "We arrive at our final formula:\n",
    "\n",
    "$A_{t}=\\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} Q(a)+\\sqrt{\\frac{2 \\log t}{N_{t}(a)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of UCB in Action\n",
    "\n",
    "To demonstrate the theory that we just worked through we have a simple example written below. The context for this example is exactly the MAB problem where we have n_bandits (number of slot machines) and money (assume $1 per play) which limits the number of rounds we play.\n",
    "\n",
    "In this example the user is asked to input a reward, 0 or 1 to simulate a successful encounter with a given bandit that has been determined by the UCB algortithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"Class for running a toy example of MAB UCB1 problem.\n",
    "    \n",
    "    :param money: How much money the gambler has to spend.\n",
    "    :type money: int\n",
    "    :param n_bandits: How many bandits there are to play.\n",
    "    :type n_bandits: int\n",
    "    \"\"\"\n",
    "    def __init__(self, money, n_bandits):\n",
    "        self.money = money\n",
    "        self.n_bandits = n_bandits\n",
    "        self.bandits_played = []\n",
    "        self.bandits_returns = {x:0 for x in range(self.n_bandits)}\n",
    "        self.n_selections = {x:0 for x in range(self.n_bandits)}\n",
    "        self.total_return = 0\n",
    "        self.bounds = {x:[] for x in range(self.n_bandits)}\n",
    "        \n",
    "    def calc_ucb(self, i, t):\n",
    "        if self.n_selections[i] > 0:\n",
    "            Qa = self.bandits_returns[i] / self.n_selections[i]\n",
    "            Ut = math.sqrt(2 * math.log(t + 1) / self.n_selections[i])\n",
    "            ub = Qa + Ut\n",
    "            print('Upper bound: ', ub)\n",
    "            self.bounds[i].append(ub)\n",
    "        else:\n",
    "            ub = 10\n",
    "            self.bounds[i].append(ub)\n",
    "        return {i:ub}\n",
    "        \n",
    "    def play_round(self, t):\n",
    "        res = {k: v for d in [self.calc_ucb(x, t) for x in range(self.n_bandits)] for k, v in d.items()}\n",
    "        bandit = max(res, key=lambda v: res[v])\n",
    "        self.bandits_played.append(bandit)\n",
    "        print('Bandit: ', bandit)\n",
    "        self.n_selections[bandit] += 1\n",
    "        cash_return = int(input())\n",
    "        self.bandits_returns[bandit] += cash_return\n",
    "        self.total_return += cash_return\n",
    "        \n",
    "    def run(self):\n",
    "        [self.play_round(x) for x in range(self.money)]\n",
    "        print(f'Bandit {max(self.bandits_returns, key=lambda v: self.bandits_returns[v])} was the optimal machine.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB1(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.177410022515475\n",
      "Bandit:  1\n",
      "0\n",
      "Upper bound:  2.4823038073675114\n",
      "Upper bound:  1.4823038073675112\n",
      "Bandit:  2\n",
      "0\n",
      "Upper bound:  2.6651092223153956\n",
      "Upper bound:  1.6651092223153954\n",
      "Upper bound:  1.6651092223153954\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.26863624117952\n",
      "Upper bound:  1.7941225779941015\n",
      "Upper bound:  1.7941225779941015\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.0929347248663586\n",
      "Upper bound:  1.8930184728248454\n",
      "Upper bound:  1.8930184728248454\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  1.9863848511243756\n",
      "Upper bound:  1.9727697022487511\n",
      "Upper bound:  1.9727697022487511\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  1.9120178817720266\n",
      "Upper bound:  2.039333980337618\n",
      "Upper bound:  2.039333980337618\n",
      "Bandit:  1\n",
      "0\n",
      "Upper bound:  1.9374912431241627\n",
      "Upper bound:  1.4823038073675112\n",
      "Upper bound:  2.09629414793641\n",
      "Bandit:  2\n",
      "0\n",
      "Upper bound:  1.9597051824376162\n",
      "Upper bound:  1.5174271293851465\n",
      "Upper bound:  1.5174271293851465\n",
      "Bandit:  0\n",
      "1\n",
      "Bandit 0 was the optimal machine.\n"
     ]
    }
   ],
   "source": [
    "ucb.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we see that bandit 0 was favoured for reward, although this example has a very limited $t$, we see that our algorithm picks up on the fact that bandit 0 has the best yield. We can also see that there is continued exploration of the other bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another demonstration, we will observe what happens if there are multiple high yeilding bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.177410022515475\n",
      "Bandit:  1\n",
      "1\n",
      "Upper bound:  2.4823038073675114\n",
      "Upper bound:  2.4823038073675114\n",
      "Bandit:  2\n",
      "0\n",
      "Upper bound:  2.6651092223153956\n",
      "Upper bound:  2.6651092223153956\n",
      "Upper bound:  1.6651092223153954\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.26863624117952\n",
      "Upper bound:  2.7941225779941012\n",
      "Upper bound:  1.7941225779941015\n",
      "Bandit:  1\n",
      "1\n",
      "Upper bound:  2.33856619904585\n",
      "Upper bound:  2.33856619904585\n",
      "Upper bound:  1.8930184728248454\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.1389791186424545\n",
      "Upper bound:  2.3949588341794583\n",
      "Upper bound:  1.9727697022487511\n",
      "Bandit:  1\n",
      "1\n",
      "Upper bound:  2.177410022515475\n",
      "Upper bound:  2.177410022515475\n",
      "Upper bound:  2.039333980337618\n",
      "Bandit:  0\n",
      "1\n",
      "Upper bound:  2.048147073968205\n",
      "Upper bound:  2.2102959906117237\n",
      "Upper bound:  2.09629414793641\n",
      "Bandit:  1\n",
      "1\n",
      "Upper bound:  2.0729830131446736\n",
      "Upper bound:  2.0729830131446736\n",
      "Upper bound:  2.145966026289347\n",
      "Bandit:  2\n",
      "0\n",
      "Bandit 0 was the optimal machine.\n"
     ]
    }
   ],
   "source": [
    "ucb2 = UCB1(10, 3)\n",
    "ucb2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that both 0 and 1 are superior, so the algorthim favors these, but still visits bandit 2 for continued exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 4, 2: 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucb2.bandits_returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
